# main.py
"""
çº¢å¤–å°ç›®æ ‡æ£€æµ‹ä¸»è®­ç»ƒè„šæœ¬ - å®Œæ•´ç‰ˆæœ¬
"""

import argparse
import torch
import torch.optim as optim
from torch.utils.data import DataLoader

from config import get_default_config, print_config
from datasets.infrared_small_target import build_infrared_dataset, collate_fn
from models.deformable_detr import DeformableDETR
from models.criterion import SetCriterion, HungarianMatcher
from engine.trainer import Trainer
from util.scheduler import WarmupCosineSchedule


def setup_config(args):
    """è®¾ç½®é…ç½®"""
    config = get_default_config()
    
    # æ›´æ–°é…ç½®å‚æ•°
    config.model.backbone = args.backbone
    config.model.hidden_dim = args.hidden_dim
    config.model.num_queries = args.num_queries
    config.data.batch_size = args.batch_size
    config.data.dataset_root = args.dataset_root
    config.train.lr = args.lr
    config.train.epochs = args.epochs
    config.train.weight_decay = args.weight_decay
    config.experiment.output_dir = args.output_dir
    config.experiment.device = args.device
    
    return config


def build_model(config):
    """æ„å»ºæ¨¡å‹"""
    print("æ„å»ºDeformable DETRæ¨¡å‹...")
    
    model = DeformableDETR(config.model)
    return model


def build_criterion(config):
    """æ„å»ºæŸå¤±å‡½æ•°"""
    print("æ„å»ºæŸå¤±å‡½æ•°...")
    
    # åŒˆç‰™åˆ©åŒ¹é…å™¨
    matcher = HungarianMatcher(
        cost_class=1.0,
        cost_bbox=5.0,
        cost_giou=2.0
    )
    
    # æŸå¤±æƒé‡
    weight_dict = {
        'loss_ce': 1.0,
        'loss_bbox': 5.0,
        'loss_giou': 2.0
    }
    
    # æŸå¤±å‡½æ•°
    losses = ['labels', 'boxes']
    
    criterion = SetCriterion(
        num_classes=config.model.num_classes,
        matcher=matcher,
        weight_dict=weight_dict,
        eos_coef=0.1,
        losses=losses
    )
    
    return criterion


def build_dataloaders(config):
    """æ„å»ºæ•°æ®åŠ è½½å™¨"""
    print("æ„å»ºæ•°æ®åŠ è½½å™¨...")
    
    train_dataset = build_infrared_dataset(config, is_train=True)
    val_dataset = build_infrared_dataset(config, is_train=False)
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.data.batch_size,
        shuffle=True,
        num_workers=config.data.num_workers,
        collate_fn=collate_fn,
        pin_memory=config.data.pin_memory,
        drop_last=config.data.drop_last
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.data.batch_size,
        shuffle=False,
        num_workers=config.data.num_workers,
        collate_fn=collate_fn,
        pin_memory=config.data.pin_memory
    )
    
    print(f"è®­ç»ƒé›†: {len(train_dataset)} å¼ å›¾åƒ")
    print(f"éªŒè¯é›†: {len(val_dataset)} å¼ å›¾åƒ")
    
    return train_loader, val_loader


def build_optimizer(config, model):
    """æ„å»ºä¼˜åŒ–å™¨"""
    
    # å‚æ•°åˆ†ç»„ï¼šéª¨å¹²ç½‘ç»œå’Œå…¶ä»–å‚æ•°
    param_dicts = [
        {
            "params": [p for n, p in model.named_parameters() 
                      if "backbone" not in n and p.requires_grad],
            "lr": config.train.lr,
        },
        {
            "params": [p for n, p in model.named_parameters() 
                      if "backbone" in n and p.requires_grad],
            "lr": config.train.lr * 0.1,  # éª¨å¹²ç½‘ç»œå­¦ä¹ ç‡æ›´ä½
        },
    ]
    
    optimizer = optim.AdamW(
        param_dicts, 
        lr=config.train.lr,
        weight_decay=config.train.weight_decay
    )
    
    return optimizer


def main():
    parser = argparse.ArgumentParser(description='çº¢å¤–å°ç›®æ ‡æ£€æµ‹è®­ç»ƒ')
    parser.add_argument('--backbone', default='resnet50', help='éª¨å¹²ç½‘ç»œ')
    parser.add_argument('--hidden_dim', type=int, default=256, help='éšè—ç»´åº¦')
    parser.add_argument('--num_queries', type=int, default=100, help='æŸ¥è¯¢æ•°é‡')
    parser.add_argument('--batch_size', type=int, default=4, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--dataset_root', default='datasets/RGBT-Tiny', help='æ•°æ®é›†æ ¹è·¯å¾„')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-4, help='æƒé‡è¡°å‡')
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--output_dir', default='outputs', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--device', default='cuda', help='è®­ç»ƒè®¾å¤‡')
    
    args = parser.parse_args()
    
    # è®¾ç½®é…ç½®
    config = setup_config(args)
    print_config(config)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(config.experiment.device)
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ„å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader = build_dataloaders(config)
    
    # æ„å»ºæ¨¡å‹
    model = build_model(config)
    model = model.to(device)
    
    # æ„å»ºæŸå¤±å‡½æ•°
    criterion = build_criterion(config)
    criterion = criterion.to(device)
    
    # æ„å»ºä¼˜åŒ–å™¨
    optimizer = build_optimizer(config, model)
    
    # æ„å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    total_steps = len(train_loader) * config.train.epochs
    warmup_steps = len(train_loader) * 5  # 5ä¸ªepochçš„warmup
    lr_scheduler = WarmupCosineSchedule(optimizer, warmup_steps, total_steps)
    
    # æ„å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        criterion=criterion,
        optimizer=optimizer,
        lr_scheduler=lr_scheduler,
        device=device,
        output_dir=config.experiment.output_dir
    )
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\næ¨¡å‹ä¿¡æ¯:")
    print(f"  æ€»å‚æ•°é‡: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nå¼€å§‹è®­ç»ƒ...")
    trainer.train(
        train_loader=train_loader,
        val_loader=val_loader,
        start_epoch=0,
        total_epochs=config.train.epochs,
        eval_interval=5,
        save_interval=10
    )
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æ¨¡å‹ä¿å­˜åœ¨: {config.experiment.output_dir}")


if __name__ == '__main__':
    main()